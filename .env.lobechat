# =============================================================================
# LOBECHAT CORE SETTINGS
# =============================================================================
# Values here keep the TypeScript frontend aligned with the existing LightRAG
# backend services. Override sensitive entries via `.env` or secrets rather than
# committing them directly if this file is tracked in VCS.

# Redis-backed storage (separate logical DBs from existing clients).
# docker-compose overrides these with password-included URLs when the stack runs.
DATABASE_URL=redis://kv:6379/2
REDIS_URL=redis://kv:6379/3

# LightRAG OpenAI-compatible endpoint exposed inside the Docker network
OLLAMA_PROXY_URL=http://api.${PUBLISH_DOMAIN}

# Domain-based configuration eliminates CORS issues entirely
# All services are served under subdomains of the same domain
# No need for server mode when using proper domain setup
# NEXT_PUBLIC_SERVICE_MODE=server

# LightRAG Ollama-compatible API via subdomain (eliminates CORS)
#OPENAI_PROXY_URL=https://api.${PUBLISH_DOMAIN}

# Alternative: Internal proxy for development
# OPENAI_PROXY_URL=http://rag:9621

# Optional access password for the UI; leave blank to disable the gate
LOBECHAT_ACCESS_CODE=

# Basic branding and default behaviour
NEXT_PUBLIC_APP_NAME="LobeChat + LightRAG"
DEFAULT_AGENT_CONFIG={"model":"lightrag:latest","systemRole":"AI assistant with graph knowledge"}

# Increase V8 heap ceiling so Next.js doesn't OOM during startup (overridden in compose)
NODE_OPTIONS=--max_old_space_size=2048

# Opt-out of analytics and integrations we do not use locally
ENABLE_OAUTH_SSO=false
ENABLE_LANGFUSE=false
TELEMETRY_ENABLED=false
LOG_LEVEL=debug

# Additional debug configuration
DEBUG=*
NEXT_PUBLIC_DEBUG=true
NODE_ENV=development
