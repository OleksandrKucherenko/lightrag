name: lightrag

x-logging-default: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

x-healthcheck-default: &default-healthcheck
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 30s

x-deploy-small: &deploy-small
  resources:
    limits:
      memory: 1G
      cpus: '0.5'
    reservations:
      memory: 512M
      cpus: '0.25'

x-deploy-medium: &deploy-medium
  resources:
    limits:
      memory: 4G
      cpus: '2.0'
    reservations:
      memory: 2G
      cpus: '1.0'

x-deploy-large: &deploy-large
  resources:
    limits:
      memory: 8G
      cpus: '4.0'
    reservations:
      memory: 4G
      cpus: '2.0'

x-ulimits-unlimited: &ulimits-unlimited
  memlock:
    soft: -1
    hard: -1
  nofile:
    soft: 65536
    hard: 65536

x-volume-bind: &volume-bind
  driver: local
  driver_opts: &volume-bind-opts
    type: none
    o: bind

services:
  # Caddy Reverse Proxy with Docker Provider
  proxy:
    # ref1: https://hub.docker.com/_/caddy
    # ref2: https://hub.docker.com/r/lucaslorentz/caddy-docker-proxy, https://github.com/lucaslorentz/caddy-docker-proxy
    image: lucaslorentz/caddy-docker-proxy:latest
    container_name: proxy
    restart: unless-stopped
    ports:
      - 80:80
      - 443:443/tcp
      - 443:443/udp
    env_file:
      - .env
      - .env.caddy
    volumes:
      # access to docker socket for capturing services config
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - frontend
      - backend
    labels:
      caddy: "https://${PUBLISH_DOMAIN}"
      caddy.respond: "/health \"OK\" 200"
      caddy.respond_1: "/debug \"Environment: local.dev development\" 200"
      caddy.respond_2: "/* \"LightRAG Local Development Stack\" 200"
    healthcheck:
      <<: *default-healthcheck
      test: [ "CMD", "/bin/caddy", "validate", "--config", "/config/caddy/Caddyfile" ]
    deploy: *deploy-small
    logging: *default-logging

  # LazyDocker Web UI - Docker Fleet Management
  monitor:
    # ref1: https://hub.docker.com/r/mosswill/isaiah
    # ref2: https://github.com/will-moss/isaiah
    image: ghcr.io/will-moss/isaiah:latest
    container_name: monitor
    restart: unless-stopped
    env_file:
      - .env
      - .env.monitoring
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - frontend
      - backend
    ports:
      - 3080:3000
    depends_on:
      - proxy
    labels:
      # Caddy labels for Isaiah web interface
      caddy: "https://monitor.${PUBLISH_DOMAIN}"
      caddy.reverse_proxy: "{{upstreams 3000}}"
      caddy.basic_auth: "*"
      caddy.basic_auth.admin: "${MONITOR_BASIC_AUTH_HASH}"
    healthcheck:
      <<: *default-healthcheck
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000" ]
    deploy: *deploy-small
    logging: *default-logging

  # Redis - KV Storage & Document Status Storage
  kv:
    # ref: https://hub.docker.com/_/redis
    image: redis:8-alpine
    container_name: kv
    restart: unless-stopped
    labels:
      # Caddy labels for Redis web interface
      caddy: "https://kv.${PUBLISH_DOMAIN}"
      caddy.reverse_proxy: "{{upstreams 6379}}"
    depends_on:
      - proxy
    env_file:
      - .env
      - .env.databases
    ports:
      - 6379:6379
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - backend
    healthcheck:
      <<: *default-healthcheck
      test: [ "CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping" ]
    deploy: *deploy-small
    logging: *default-logging

  # Memgraph - Graph Storage
  graph:
    # ref: https://hub.docker.com/r/memgraph/memgraph-mage
    image: memgraph/memgraph-mage:latest
    container_name: graph
    restart: unless-stopped
    env_file:
      - .env
      - .env.databases
    # No authentication for internal services
    command: >
      --log-level=WARNING --memory-limit=4096 --data-recovery-on-startup=true --telemetry-enabled=false
    volumes:
      - memgraph_data:/var/lib/memgraph
      - memgraph_log:/var/log/memgraph
    networks:
      - backend
    ports:
      - 7687:7687
    healthcheck:
      <<: *default-healthcheck
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/7687' || exit 1
      # test: [ "CMD-SHELL", "echo 'RETURN 0;' | mgconsole --host 127.00.0.1 --port 7687 || exit 1" ]
    ulimits: *ulimits-unlimited
    deploy: *deploy-large
    logging: *default-logging

  # Memgraph Lab (Web UI for graph visualization)
  graph-ui:
    # ref: https://hub.docker.com/r/memgraph/lab
    image: memgraph/lab:3.5.0
    container_name: graph-ui
    restart: unless-stopped
    env_file:
      - .env
      - .env.databases
    environment:
      - QUICK_CONNECT_MG_HOST=graph
    networks:
      - backend
    ports:
      - 3090:3000
    labels:
      # Caddy labels for automatic subdomain routing
      caddy: "https://graph.${PUBLISH_DOMAIN}"
      caddy.reverse_proxy: "{{upstreams 3000}}"
    depends_on:
      - proxy
      - graph
    healthcheck:
      <<: *default-healthcheck
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000" ]
    deploy: *deploy-small
    logging: *default-logging

  # Qdrant - Vector Storage
  vectors:
    # ref: https://hub.docker.com/r/qdrant/qdrant
    image: qdrant/qdrant:latest
    container_name: vectors
    restart: unless-stopped
    depends_on:
      - proxy
    env_file:
      - .env
      - .env.databases
    volumes:
      - qdrant_storage:/qdrant/storage
      - qdrant_snapshots:/qdrant/snapshots
    networks:
      - backend
    ports:
      - 6333:6333
      - 6334:6334
    labels:
      # Caddy labels for automatic subdomain routing
      caddy: "https://vector.${PUBLISH_DOMAIN}"
      caddy.reverse_proxy: "{{upstreams 6333}}"
    healthcheck:
      <<: *default-healthcheck
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/6333' || exit 1
    deploy: *deploy-medium
    logging: *default-logging

  # Light RAG
  rag:
    # ref: https://github.com/HKUDS/LightRAG/pkgs/container/lightrag
    image: ghcr.io/hkuds/lightrag:latest
    container_name: rag
    restart: unless-stopped
    depends_on:
      - proxy
      - kv
      - vectors
      - graph
    env_file:
      - .env
      - .env.lightrag
    environment:
      - LLM_BINDING_API_KEY=${LLM_BINDING_API_KEY}
      - EMBEDDING_BINDING_API_KEY=${EMBEDDING_BINDING_API_KEY}
    volumes:
      - lightrag_storage:/app/data/rag_storage
      - lightrag_inputs:/app/data/inputs
      - lightrag_logs:/app/logs
    networks:
      - frontend
      - backend
    ports:
      - 9080:9621
    labels:
      # Caddy labels for automatic subdomain routing
      caddy: "https://rag.${PUBLISH_DOMAIN}"
      caddy.reverse_proxy: "{{upstreams 9621}}"
    healthcheck:
      <<: *default-healthcheck
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/9621' || exit 1
    deploy: *deploy-medium
    logging: *default-logging


  # LobeChat Frontend
  lobechat:
    # ref: https://github.com/lobehub/lobe-chat
    image: lobehub/lobe-chat:latest
    container_name: lobechat
    restart: unless-stopped
    depends_on:
      - proxy
      - rag
      - kv
    env_file:
      - .env
      - .env.lobechat
    environment:
      - DATABASE_URL=redis://:${REDIS_PASSWORD}@kv:6379/2
      - REDIS_URL=redis://:${REDIS_PASSWORD}@kv:6379/3
      - OLLAMA_PROXY_URL=http://rag:9621/v1
      - ACCESS_CODE=${LOBECHAT_ACCESS_CODE:-}
      - NODE_OPTIONS=--max_old_space_size=2048
    volumes:
      - lobechat_data:/app/.next
    networks:
      - frontend
      - backend
    ports:
      - 3210:3210
    labels:
      # Caddy labels for automatic subdomain routing
      caddy: "https://lobechat.${PUBLISH_DOMAIN}"
      caddy.reverse_proxy: "{{upstreams 3210}}"
    healthcheck:
      <<: *default-healthcheck
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3210/" ]
    deploy: *deploy-small
    logging: *default-logging

networks:
  frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1
  backend:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.21.0.0/24
          gateway: 172.21.0.1

volumes:
  # Caddy
  caddy_data:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      # Caddy will automatically create sub-folder 
      device: ./docker/data
  caddy_config:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      # Caddy will automatically create sub-folder 
      device: ./docker/etc
  # Application Data
  redis_data:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/data/redis
  # Graph
  memgraph_data:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/data/memgraph
  memgraph_log:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/logs/memgraph
  # QDRANT Vector
  qdrant_storage:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/data/qdrant/storage
  qdrant_snapshots:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/data/qdrant/snapshots
  # Rag
  lightrag_storage:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/data/lightrag/storage
  lightrag_inputs:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/data/lightrag/inputs
  lightrag_logs:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/logs/lightrag
  # LobeChat Data
  lobechat_data:
    <<: *volume-bind
    driver_opts:
      <<: *volume-bind-opts
      device: ./docker/data/lobechat
